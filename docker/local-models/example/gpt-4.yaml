#context_size: 32768
#context_size: 16384
context_size: 8192
#context_size: 4096
#context_size: 1024
name: gpt-4
roles:
  function: 'Function Result:'
  assistant_function_call: 'Function Call:'
  assistant: 'Assistant:'
  user: 'User:'
  system: 'System:'
parameters:
  model: v2-Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B-Q8_0.gguf
  temperature: 0.2
  top_k: 80
  top_p: 0.7
  repetition_penality: 1
template:
  chat: chat
  completion: completion
gpu_layers: 40
backend: llama
threads: 16

# Set it to 8 for llama2 70b
ngqa: 8
## LLAMA specific options
# Enable F16 if backend supports it
f16: true
# Enable debugging
debug: true
# Enable embeddings
#embeddings: true
# Mirostat configuration (llama.cpp only)
mirostat_eta: 0.8
mirostat_tau: 0.9
mirostat: 1
# Enable memory lock
mmlock: true
# GPU setting to split the tensor in multiple parts and define a main GPU
# see llama.cpp for usage
#tensor_split: ""
#main_gpu: ""
# Define a prompt cache path (relative to the models)
#prompt_cache_path: "/tpm/prompt_cache"
# Cache all the prompts
#prompt_cache_all: true
# Read only
#prompt_cache_ro: false
# Enable mmap
mmap: true
# Enable low vram mode (GPU only)
#low_vram: true
# Set NUMA mode (CPU only)
numa: true
# Lora settings
#lora_adapter: "/path/to/lora/adapter"
#lora_base: "/path/to/lora/base"
# Disable mulmatq (CUDA)
#no_mulmatq: true

cuda: true
