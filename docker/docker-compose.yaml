name: agicr

networks:
    agi-cr-net:
        driver: bridge

x-global: &common
    logging:
        driver: "json-file"
        options:
            max-size: 50M
    networks:
        - agi-cr-net

services:
#    ollama:
#        <<: *common
#        image: "ollama/ollama"
#        ports:
#            - 11434:11434
#        cap_add:
#            - SYS_RESOURCE
#        ulimits:
#            stack: 67108864
#            memlock: -1
#        ipc: host
#        environment:
#            CUDA_VISIBLE_DEVICES: 0,1
#            OLLAMA_KEEP_ALIVE: "48h"
#            HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
#            OLLAMA_MODEL: "qwen2.5:32b"
#        deploy:
#            resources:
#                reservations:
#                    devices:
#                        -   driver: nvidia
#                            count: all
#                            capabilities: [ gpu ]
#        volumes:
#            - ./local-models/huggingface:/root/.cache/huggingface
#            - ./data/ollama:/root/.ollama
#        # run manually after start  `docker-compose exec ollama bash -c 'ollama pull qwen2.5:32b && ollama run qwen2.5:32b'`

    localai:
        <<: *common
        image: "vllm/vllm-openai:latest"
        environment:
            CUDA_VISIBLE_DEVICES: 0,1
            OMP_NUM_THREADS: 1
            HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
        volumes:
            - ./local-models/huggingface:/root/.cache/huggingface
        command: [
#            "--model", "Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4",
            "--model", "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4",
            "--port", "80",
            "--host", "0.0.0.0",
            "--tensor-parallel-size", '2',
            "--max-model-len", '12000', # 8192
            "--gpu-memory-utilization", '1',
            "--served-model-name", "localai",
            "--enable-auto-tool-choice",
            "--tool-call-parser", "hermes",
            "--gpu-memory-utilization", '0.7',
#            "--enforce-eager",
#            '--use-v2-block-manager',
#            "--chat-template", "tool_use",
#            "--trust-remote-code",
            "--pipeline-parallel-size", '1'
        ]
        ports:
            - 9088:80
        cap_add:
            - SYS_RESOURCE
        ulimits:
            stack: 67108864
            memlock: -1
        ipc: host
        deploy:
            resources:
                reservations:
                    devices:
                        -   driver: nvidia
                            count: all
                            capabilities: [ gpu ]

#    localai:
#        <<: *common
#        build:
#            context: ./images/llm
#            dockerfile: ./Dockerfile
#        environment:
#            CUDA_VISIBLE_DEVICES: 0,1
#        volumes:
#            - ./local-models:/models:cached
#        # see https://github.com/ggerganov/llama.cpp/tree/master/examples/server
#        command: [
##            "--api-key", "sk-local",
##            "--chat-template", "llama3",
##            "-m", "/models/Meta-Llama-3.1-70B-Instruct-Q4_K_L.gguf",
#            "--chat-template", "chatml",
#            "-m", "/models/qwen2.5-72b-instruct-q4_k_m-00001-of-00012.gguf",
#            "--alias", "localai",
#            "--port", "80",
#            "--host", "0.0.0.0",
#            "--n-gpu-layers", "100",
#          #            "--cache-type-k", "q8_0",
#          #            "--cache-type-v", "q8_0", # requires flash_attn
#            "--mlock",
#          # 8B model
#          #            "--no-slots", # slots state may contain user data, prompts included.
#          #            "--log-disable",
#          #            "--n-predict", "1024", #maximum tokens to predict
#          #            "--ctx-size", "32768", # KV cache size, T tokens (prompt + generated), set to T*Parallel in order to handle the worst-case
#          #            "--parallel", "10", # we have 8*4096=32768 tokens, but assume no max tokens in parallel, so give 2 more
#          # 70B model
#            "--n-predict", "1024", #maximum tokens to predict
#            "--ctx-size", "2500", # KV cache size, T tokens (prompt + generated), set to T*Parallel in order to handle the worst-case
#            "--tensor-split", "0.46,0.54",
#            "--parallel", "1",
#        ]
#        ports:
#            - 9088:80
#        cap_add:
#            - SYS_RESOURCE
#        ulimits:
#            stack: 67108864
#            memlock: -1
#        ipc: host
#        deploy:
#            resources:
#                reservations:
#                    devices:
#                        -   driver: nvidia
#                            count: all
#                            capabilities: [ gpu ]

    redis:
        <<: *common
        image: "redis:7-alpine"
        volumes:
            - ./data/redis:/data

    php: &php
        <<: *common
        build:
            context: ../.
            dockerfile: ./docker/images/php/Dockerfile-local
        volumes:
            - ../aider/projects:/aider/projects
            - ./aider/config:/aider/config
            - ../backend:/app
            - ./images/php/php.ini:/usr/local/etc/php/conf.d/custom-php.ini
            - ./images/php/docker-php-ext-xdebug-add.ini:/usr/local/etc/php/conf.d/docker-php-ext-xdebug2.ini
            - ./images/php/docker-php-ext-xdebug-ip.ini:/usr/local/etc/php/conf.d/docker-php-ext-xdebug3.ini
            - $SSH_AUTH_SOCK:/ssh-agent # Forward host machine SSH key to docker
            - ~/.composer:/home/commanduser/.composer # Forward composer cache and credentials
        environment: &php-env
            PHP_IDE_CONFIG: serverName=app.local
            SSH_AUTH_SOCK: /ssh-agent
            COMPOSER_MEMORY_LIMIT: 8G
            DATABASE_URL: mysql://root:app@mysql:3306/app?serverVersion=8.3.0
            REDIS_DSN: redis://redis:6379
            PROJECT_TASKS_LIMIT: 10000
            GLOBAL_TASKS_LIMIT: 100000
        ports:
            - 9089:80
        depends_on:
            - mysql
            - redis

    cron:
        <<: *php
        user: root
        ports: []
        environment:
            <<: *php-env
        command: ["-f", "-d", "3", "-l", "3"]
        entrypoint: "/usr/sbin/crond"
#        entrypoint: "sh"
#        command: ["-c exit"]
#        command: ["-f"]

    probot:
        <<: *common
        build:
            context: ../probot
            dockerfile: Dockerfile
        volumes:
            - ../probot:/usr/src/app
            - ./data/cache/hub:/file-cache-storage
        env_file:
            - ../probot/.env
        depends_on:
            - php

    mysql:
        <<: *common
        image: "mysql:8"
        environment:
            MYSQL_ROOT_PASSWORD: app
            MYSQL_DATABASE: app
            MYSQL_USER: app
            MYSQL_PASSWORD: app
        volumes:
            - ./data/mysql:/var/lib/mysql

    phpmyadmin:
        <<: *common
        image: "phpmyadmin/phpmyadmin"
        ports:
            - 9081:80
        environment:
            MYSQL_ROOT_PASSWORD: app
            MYSQL_DATABASE: app
            MYSQL_USER: app
            MYSQL_PASSWORD: app
            PMA_HOST: mysql
            UPLOAD_LIMIT: 4G
        depends_on:
            - mysql

#    nextjs:
#        <<: *common
#        build:
#            context: ../nextjs/
#            dockerfile: ../docker/images/nextjs/Dockerfile
#        ports:
#            - 9080:3000
#        user: "1000:1000"
#        volumes:
#            - ../nextjs:/app
#        command: [ "npm", "run", "dev" ]
#        environment:
#            - NODE_ENV=development

#
#    lcpp:
#        <<: *common
#        image: "ghcr.io/ggerganov/llama.cpp:server-cuda"
#        env_file:
#            - .env.localai
#        volumes:
#            - ./data/models:/models:cached
#            - ./data/images/:/tmp/generated/images/
#            - ./data/huggingface/:/usr/local/huggingface/
#        command: [ "-m", "/models/codellama-70b-instruct.Q4_K_M.gguf", "--port", "8000", "--host", "0.0.0.0", "-n", "16384", "-c", "16384", "--n-gpu-layers", "30", "--mlock" ]
#        ports:
#            - 9099:8000
#        cap_add:
#            - SYS_RESOURCE
#        ulimits:
#            memlock: -1
#        deploy:
#            resources:
#                reservations:
#                    devices:
#                        -   driver: nvidia
#                            count: all
#                            capabilities: [ gpu ]
